{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416a0ab4",
   "metadata": {},
   "source": [
    "# How Naive classification Model works explain with example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23411ca0",
   "metadata": {},
   "source": [
    "* Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem.\n",
    "* It is based on Bayes Theorem:\n",
    "    Bayes theorem is also known as Bayes Rule or Bayes law, which is used to determine the probability of a hypothesis with\n",
    "    prior knowledge. It depends on the conditional probability.\n",
    "    \n",
    "    P(A|B)=P(B|A)P(A)/P(B)\n",
    "\n",
    "* Example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:\n",
    "\n",
    "naive bayes, bayes theorem\n",
    "       \n",
    "       P(A|B)=P(B|A)P(A)/P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf187d36",
   "metadata": {},
   "source": [
    "# Which Loss Function is applicable for Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb97ed6",
   "metadata": {},
   "source": [
    "* Mean Square Error loss function is applicable for Regression\n",
    "* Mean Square Error (MSE) is defined as Mean or Average of the square of the difference between actual and estimated values.\n",
    "* Formula:\n",
    "* MSE or loss function = 1/n*sum((yp-y)^2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99a904",
   "metadata": {},
   "source": [
    "# Which Loss Function is applicable for Classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6608ad9",
   "metadata": {},
   "source": [
    "* Cross Entropy Loss Function is applicable for classification.\n",
    "* When optimizing classification models, cross-entropy is commonly employed as a loss function. The logistic regression \n",
    "  technique can be utilized for classification problems.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235cbf6d",
   "metadata": {},
   "source": [
    "# How KNN works explain with example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f0b9c",
   "metadata": {},
   "source": [
    "* Step-1: Select the number K of the neighbor\n",
    "\n",
    "* Step-2: Calculate the Euclidean distance of K number of neighbors\n",
    "\n",
    "* Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n",
    "\n",
    "* Step-4: Among these k neighbors, count the number of the data points in each category.\n",
    "\n",
    "* Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n",
    "\n",
    "* Step-6: Our model is ready.\n",
    "\n",
    "* Example:\n",
    "\n",
    "Several parties compete in an election in a democratic country like India. Parties compete for voter support during election campaigns. The public votes for the candidate with whom they feel more connected.\n",
    "\n",
    "When the votes for all of the candidates have been recorded, the candidate with the most votes is declared as the election’s winner.\n",
    "\n",
    "KNN employs a mean/average method for predicting the value of new data. Based on the value of K, it would consider all of the nearest neighbours.\n",
    "\n",
    "The algorithm attempts to calculate the mean for all the nearest neighbours’ values until it has identified all the nearest neighbours within a certain range of the K value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df4000",
   "metadata": {},
   "source": [
    "# In your opinion which model is better in Naïve and KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67181572",
   "metadata": {},
   "source": [
    "* Naive Bayes model is better than KNN because it is faster.\n",
    "* KNN is Lazy model because euclidean distance is calculated for every features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef7d09",
   "metadata": {},
   "source": [
    "# What is different between Supervised and Unsupervised Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa1034",
   "metadata": {},
   "source": [
    "* A key difference between supervised and unsupervised learning algorithms is that supervised learning algorithms require labels\n",
    "  or categories for the training dataset, as opposed to unsupervised learning algorithms where the goal is to cluster data \n",
    "  points into discrete groups.\n",
    "* Supervised learning is typically done in the context of classification when we want to map the input to output labels, or \n",
    "  regression when we want to map the input to continuous output. Common algorithms in supervised learning include logistic \n",
    "  regression, naive Bayes, support vector machines, artificial neural networks, and random forests. When conducting supervised \n",
    "  learning, the main considerations are model complexity and the bias-variance tradeoff. Note that both of these are interrelated.\n",
    "* Unsupervised learning algorithms, such as K-means clustering, assume that the input dataset is distributed according to some unknown underlying statistical distribution. The goal of unsupervised learning algorithms is to learn the true nature of this distribution. It attempts to find clusters or groups in the data without any labels for what constitutes a \"cluster\" or \"group\". Unsupervised learning is also very useful in exploratory analysis because it can automatically identify structure in data. In situations where it is either impossible for a human to propose trends in the data, unsupervised learning can provide initial insights that can then be used to test individual hypotheses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e55665",
   "metadata": {},
   "source": [
    "# How can we avoid Overfiting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e63421",
   "metadata": {},
   "source": [
    "* Cross-validation. Cross-validation is a powerful preventative measure against overfitting. ...\n",
    "* Train with more data. It won't work every time, but training with more data can help algorithms detect the signal better. ...\n",
    "* Remove features. ...\n",
    "* Early stopping. ...\n",
    "* Regularization. ...\n",
    "* Ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9b1ce",
   "metadata": {},
   "source": [
    "# What is the Problem in Under – fiting Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae74d0d",
   "metadata": {},
   "source": [
    "* Underfitting – High bias and low variance.\n",
    "* Underfitting destroys the accuracy of our machine learning model\n",
    "* Techniques to reduce underfitting: \n",
    "1. Increase model complexity \n",
    "2. Increase the number of features, performing feature engineering \n",
    "3. Remove noise from the data. \n",
    "4. Increase the number of epochs or increase the duration of training to get better results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
